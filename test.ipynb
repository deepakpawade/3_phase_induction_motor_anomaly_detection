{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.fft import fft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define the chunk size and anomaly threshold\n",
    "chunk_size = 10000  # process 10000 rows at a time\n",
    "# anomaly_thresh = 0.5  # set the anomaly threshold\n",
    "\n",
    "# Read the data in chunks\n",
    "datafolder = 'data/s3Files/'\n",
    "data_files = os.listdir(datafolder)\n",
    "\n",
    "# Iterate over the files and process the data in chunks\n",
    "for file in data_files:\n",
    "    # Load the data chunk by chunk\n",
    "    for chunk in pd.read_csv(datafolder + file, header=None, chunksize=chunk_size):\n",
    "\n",
    "\n",
    "        # Generate random indices to modify\n",
    "        num_anomalies = 100\n",
    "        anomaly_indices = np.random.choice(chunk.index, num_anomalies, replace=False)\n",
    "\n",
    "        # Modify the values at the anomaly indices\n",
    "        max_deviation = 24\n",
    "        for index in anomaly_indices:\n",
    "            row = chunk.loc[index]\n",
    "            col = np.random.choice(chunk.columns)\n",
    "            deviation = max_deviation * np.random.random()\n",
    "            chunk.loc[index, col] = row[col] + deviation\n",
    "       \n",
    "        scaler = StandardScaler()\n",
    "        chunk = scaler.fit_transform(chunk)\n",
    "\n",
    "        # Process the chunk data\n",
    "        fft_data = fft(chunk, axis=1)\n",
    "        peak_freqs = np.argmax(fft_data, axis=1)\n",
    "        mean_currents = np.mean(chunk, axis=1)\n",
    "        mean_freq = np.mean(peak_freqs)\n",
    "        std_freq = np.std(peak_freqs)\n",
    "        thresh_freq = mean_freq + 3 * std_freq\n",
    "        anomalies_freq = np.where(peak_freqs > thresh_freq)[0]\n",
    "        # print(thresh_freq)\n",
    "        \n",
    "        # Check for anomalies and alert if detected\n",
    "        if len(anomalies_freq) > 0:\n",
    "            print(f\"Anomaly detected in {file}: {len(anomalies_freq)} anomalies detected in chunk {chunk.index[0]} - {chunk.index[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/s3Files/data10.txt',header=None,sep=',')\n",
    "df.drop(columns=[0,4],inplace=True)\n",
    "df.columns = ['current_1', 'current_2', 'current_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_anomalies = 10\n",
    "anomaly_indices = np.random.choice(df.index, num_anomalies, replace=False)\n",
    "for index in anomaly_indices:\n",
    "    row = df.loc[index]\n",
    "    col = np.random.choice(df.columns)\n",
    "    df.loc[index, col] = row[col] + 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ano(chunk):\n",
    "    scaler = StandardScaler()\n",
    "    chunk = scaler.fit_transform(chunk)\n",
    "\n",
    "    # Process the chunk data\n",
    "    fft_data = fft(chunk, axis=1)\n",
    "    peak_freqs = np.argmax(fft_data, axis=1)\n",
    "    mean_currents = np.mean(chunk, axis=1)\n",
    "    mean_freq = np.mean(peak_freqs)\n",
    "    std_freq = np.std(peak_freqs)\n",
    "    thresh_freq = mean_freq + 3 * std_freq\n",
    "    anomalies_freq = np.where(peak_freqs > thresh_freq)[0]\n",
    "    # print(thresh_freq)\n",
    "    \n",
    "    # Check for anomalies and alert if detected\n",
    "    if len(anomalies_freq) > 0:\n",
    "        print(f\"Anomaly detected{len(anomalies_freq)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ano(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "chunk = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_data = fft(chunk, axis=1)\n",
    "peak_freqs = np.argmax(fft_data, axis=1)\n",
    "mean_currents = np.mean(chunk, axis=1)\n",
    "mean_freq = np.mean(peak_freqs)\n",
    "std_freq = np.std(peak_freqs)\n",
    "thresh_freq = mean_freq + 2 * std_freq\n",
    "anomalies_freq = np.where(peak_freqs > thresh_freq)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "peak freqs will always be within 0,1,2 and thresh will always be something > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(peak_freqs > thresh_freq)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0398511a9cde84ab83a2fa188ff6508a33d7c0397b3581839dbbf3238a247df4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
