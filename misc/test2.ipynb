{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('data/s3Files/data12.txt',header=None,sep=',')\n",
    "df.drop(columns=[0,4],inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.columns = ['current_1', 'current_2', 'current_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create condition indicators based on MCSA and VI analysis techniques\n",
    "df['BrokenRotorBars'] = abs(df['current_1']) - abs(df['current_2'])\n",
    "df['BearingFault'] = abs(df['current_2']) - abs(df['current_3'])\n",
    "df['Eccentricity'] = abs(df['current_3']) - abs(df['current_1'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common unsupervised learning technique for anomaly detection is clustering. We can use clustering algorithms to group similar data points together and identify any data points that are significantly different from the others as potential anomalies.\n",
    "In this code, we load the current data into a Pandas dataframe, drop any NaN values, and then perform clustering using the KMeans algorithm with 10 clusters. We then use the distances between each data point and its cluster center to identify potential anomalies. Any data point whose distance to its cluster center is more than 3 standard deviations away from the mean distance is considered a potential anomaly.\n",
    "\n",
    "The anomalies variable contains the data points that are identified as potential anomalies. These data points can then be further analyzed to determine if they are truly anomalous or if they are just noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Generate random indices to modify\n",
    "num_anomalies = 10\n",
    "anomaly_indices = np.random.choice(df.index, num_anomalies, replace=False)\n",
    "\n",
    "# Modify the values at the anomaly indices\n",
    "max_deviation = 4\n",
    "for index in anomaly_indices:\n",
    "    row = df.loc[index]\n",
    "    col = np.random.choice(df.columns)\n",
    "    deviation = max_deviation * np.random.random()\n",
    "    df.loc[index, col] = row[col] + deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = (df - df.mean()) / df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_1</th>\n",
       "      <th>current_2</th>\n",
       "      <th>current_3</th>\n",
       "      <th>BrokenRotorBars</th>\n",
       "      <th>BearingFault</th>\n",
       "      <th>Eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>-1.384284</td>\n",
       "      <td>-0.811185</td>\n",
       "      <td>1.362104</td>\n",
       "      <td>-0.326259</td>\n",
       "      <td>-1.318009</td>\n",
       "      <td>1.593228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>-1.373270</td>\n",
       "      <td>0.390772</td>\n",
       "      <td>1.387678</td>\n",
       "      <td>-1.002224</td>\n",
       "      <td>-0.783149</td>\n",
       "      <td>1.605990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>1.407650</td>\n",
       "      <td>0.875966</td>\n",
       "      <td>-1.410883</td>\n",
       "      <td>0.302765</td>\n",
       "      <td>1.381638</td>\n",
       "      <td>-1.638140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>1.402143</td>\n",
       "      <td>0.931101</td>\n",
       "      <td>-1.399923</td>\n",
       "      <td>0.268341</td>\n",
       "      <td>1.399382</td>\n",
       "      <td>-1.627931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>-1.378777</td>\n",
       "      <td>-0.904916</td>\n",
       "      <td>1.380371</td>\n",
       "      <td>-0.269928</td>\n",
       "      <td>-1.373776</td>\n",
       "      <td>1.603438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>-1.367763</td>\n",
       "      <td>0.308068</td>\n",
       "      <td>1.398639</td>\n",
       "      <td>-0.952153</td>\n",
       "      <td>-0.828777</td>\n",
       "      <td>1.611095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>-1.345736</td>\n",
       "      <td>-0.987619</td>\n",
       "      <td>1.394985</td>\n",
       "      <td>-0.204209</td>\n",
       "      <td>-1.421939</td>\n",
       "      <td>1.598333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>1.413157</td>\n",
       "      <td>-0.303937</td>\n",
       "      <td>-1.425497</td>\n",
       "      <td>0.975602</td>\n",
       "      <td>0.849313</td>\n",
       "      <td>-1.650902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>-1.373270</td>\n",
       "      <td>0.269473</td>\n",
       "      <td>1.405946</td>\n",
       "      <td>-0.933376</td>\n",
       "      <td>-0.851591</td>\n",
       "      <td>1.618753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>1.407650</td>\n",
       "      <td>-0.281883</td>\n",
       "      <td>-1.443764</td>\n",
       "      <td>0.959954</td>\n",
       "      <td>0.872127</td>\n",
       "      <td>-1.661112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966</th>\n",
       "      <td>1.402143</td>\n",
       "      <td>1.085481</td>\n",
       "      <td>-1.443764</td>\n",
       "      <td>0.180716</td>\n",
       "      <td>1.500777</td>\n",
       "      <td>-1.658560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>1.374609</td>\n",
       "      <td>1.124076</td>\n",
       "      <td>-1.440111</td>\n",
       "      <td>0.143162</td>\n",
       "      <td>1.515987</td>\n",
       "      <td>-1.643245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6534</th>\n",
       "      <td>1.325048</td>\n",
       "      <td>1.173698</td>\n",
       "      <td>-1.421844</td>\n",
       "      <td>0.086832</td>\n",
       "      <td>1.526126</td>\n",
       "      <td>-1.607511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6730</th>\n",
       "      <td>-1.296175</td>\n",
       "      <td>-1.158540</td>\n",
       "      <td>1.391332</td>\n",
       "      <td>-0.079030</td>\n",
       "      <td>-1.497986</td>\n",
       "      <td>1.572809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7486</th>\n",
       "      <td>1.292008</td>\n",
       "      <td>1.212293</td>\n",
       "      <td>-1.392616</td>\n",
       "      <td>0.046149</td>\n",
       "      <td>1.523591</td>\n",
       "      <td>-1.571777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9018</th>\n",
       "      <td>1.170859</td>\n",
       "      <td>1.261915</td>\n",
       "      <td>-1.319546</td>\n",
       "      <td>-0.050865</td>\n",
       "      <td>1.495708</td>\n",
       "      <td>-1.464576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>1.165352</td>\n",
       "      <td>1.206780</td>\n",
       "      <td>-1.283011</td>\n",
       "      <td>-0.022700</td>\n",
       "      <td>1.445010</td>\n",
       "      <td>-1.436499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9548</th>\n",
       "      <td>1.143325</td>\n",
       "      <td>1.250888</td>\n",
       "      <td>-1.283011</td>\n",
       "      <td>-0.060253</td>\n",
       "      <td>1.465289</td>\n",
       "      <td>-1.426289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      current_1  current_2  current_3  BrokenRotorBars  BearingFault  \\\n",
       "1634  -1.384284  -0.811185   1.362104        -0.326259     -1.318009   \n",
       "2025  -1.373270   0.390772   1.387678        -1.002224     -0.783149   \n",
       "2221   1.407650   0.875966  -1.410883         0.302765      1.381638   \n",
       "2612   1.402143   0.931101  -1.399923         0.268341      1.399382   \n",
       "2808  -1.378777  -0.904916   1.380371        -0.269928     -1.373776   \n",
       "3199  -1.367763   0.308068   1.398639        -0.952153     -0.828777   \n",
       "3592  -1.345736  -0.987619   1.394985        -0.204209     -1.421939   \n",
       "3788   1.413157  -0.303937  -1.425497         0.975602      0.849313   \n",
       "3985  -1.373270   0.269473   1.405946        -0.933376     -0.851591   \n",
       "4181   1.407650  -0.281883  -1.443764         0.959954      0.872127   \n",
       "4966   1.402143   1.085481  -1.443764         0.180716      1.500777   \n",
       "5358   1.374609   1.124076  -1.440111         0.143162      1.515987   \n",
       "6534   1.325048   1.173698  -1.421844         0.086832      1.526126   \n",
       "6730  -1.296175  -1.158540   1.391332        -0.079030     -1.497986   \n",
       "7486   1.292008   1.212293  -1.392616         0.046149      1.523591   \n",
       "9018   1.170859   1.261915  -1.319546        -0.050865      1.495708   \n",
       "9253   1.165352   1.206780  -1.283011        -0.022700      1.445010   \n",
       "9548   1.143325   1.250888  -1.283011        -0.060253      1.465289   \n",
       "\n",
       "      Eccentricity  \n",
       "1634      1.593228  \n",
       "2025      1.605990  \n",
       "2221     -1.638140  \n",
       "2612     -1.627931  \n",
       "2808      1.603438  \n",
       "3199      1.611095  \n",
       "3592      1.598333  \n",
       "3788     -1.650902  \n",
       "3985      1.618753  \n",
       "4181     -1.661112  \n",
       "4966     -1.658560  \n",
       "5358     -1.643245  \n",
       "6534     -1.607511  \n",
       "6730      1.572809  \n",
       "7486     -1.571777  \n",
       "9018     -1.464576  \n",
       "9253     -1.436499  \n",
       "9548     -1.426289  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(df_scaled)\n",
    "\n",
    "# Get cluster labels and distances to cluster centers\n",
    "labels = kmeans.labels_\n",
    "distances = kmeans.transform(df_scaled)\n",
    "\n",
    "# Identify potential anomalies\n",
    "df_scaled[(distances > distances.mean() + 1.5 * distances.std()).any(axis=1)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices of anomalies dont match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8993, 2460, 3813, 9485, 7053, 3890, 1155, 6224, 8753, 3369],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepdesk\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('data/s3Files/data12.txt', header=None, sep=',')\n",
    "df.drop(columns=[0, 4], inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.columns = ['current_1', 'current_2', 'current_3']\n",
    "\n",
    "# Create condition indicators based on MCSA and VI analysis techniques\n",
    "df['BrokenRotorBars'] = abs(df['current_1']) - abs(df['current_2'])\n",
    "df['BearingFault'] = abs(df['current_2']) - abs(df['current_3'])\n",
    "df['Eccentricity'] = abs(df['current_3']) - abs(df['current_1'])\n",
    "\n",
    "# Generate random indices to modify\n",
    "num_anomalies = 10\n",
    "anomaly_indices = np.random.choice(df.index, num_anomalies, replace=False)\n",
    "\n",
    "# Modify the values at the anomaly indices\n",
    "max_deviation = 4\n",
    "for index in anomaly_indices:\n",
    "    row = df.loc[index]\n",
    "    col = np.random.choice(df.columns)\n",
    "    deviation = max_deviation * np.random.random()\n",
    "    df.loc[index, col] = row[col] + deviation\n",
    "\n",
    "# # Scale the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Fit the isolation forest model\n",
    "model = IsolationForest(n_estimators=200, contamination=0.001)\n",
    "model.fit(df)\n",
    "\n",
    "# Predict the anomalies\n",
    "y_pred = model.predict(df)\n",
    "indices = np.where(y_pred == -1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1647, 2529, 3280, 3478, 3882, 6404, 6802, 7772, 9139, 9182],\n",
       "       dtype=int64),\n",
       " array([2221, 2612, 4966, 5358, 6534, 6964, 7356, 7486, 7916, 8879],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(anomaly_indices),np.sort(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(anomaly_indices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " periodicity in the current readings can affect the model as it can create false positives for anomaly detection. To rectify this, we can use a technique called Fourier Transform to extract the frequency components of the data and remove the periodicity.\n",
    "\n",
    " In this code, we first apply Fourier Transform to the original data df to extract the frequency components. We then create a mask to remove the frequency components corresponding to the periodicity, and apply the mask to the frequency domain data. We then use the inverse Fourier Transform to obtain the filtered data df_filtered.\n",
    "\n",
    "Finally, we create the condition indicators based on the filtered data and train the machine learning model on the filtered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1359, 2535, 2731, 3161, 3200, 3487, 3721, 4543, 4763, 4880, 5019,\n",
       "        5588], dtype=int64),\n",
       " array([9468,  218,  490, 7216, 9140, 9045, 8862, 8780, 5724,  266],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('data/s3Files/data12.txt',header=None,sep=',')\n",
    "df.drop(columns=[0,4],inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.columns = ['current_1', 'current_2', 'current_3']\n",
    "\n",
    "# Apply Fourier Transform to remove periodicity\n",
    "from scipy.fft import fft, ifft\n",
    "\n",
    "\n",
    "# Apply bandpass filter\n",
    "df_fft = fft(df)\n",
    "freqs = np.fft.fftfreq(len(df))\n",
    "\n",
    "\n",
    "# Create mask to filter out frequencies in 0.1-0.9 Hz range\n",
    "mask = np.where((freqs > 0.1) & (freqs < 0.9), 0, 1)\n",
    "# mask = np.tile(mask, (1, 3)) # reshape mask to match df_fft shape\n",
    "\n",
    "# df_fft_filtered = df_fft * mask.reshape(-1, 1)\n",
    "# df_filtered = np.real(ifft(df_fft_filtered))\n",
    "\n",
    "df_filtered = copy.deepcopy(df.loc[mask.astype(bool)])\n",
    "\n",
    "\n",
    "# Generate random indices to modify\n",
    "num_anomalies = 10\n",
    "anomaly_indices = np.random.choice(df_filtered.index, num_anomalies, replace=False)\n",
    "# Modify the values at the anomaly indices\n",
    "max_deviation = 4\n",
    "for index in anomaly_indices:\n",
    "    row = df_filtered.loc[index]\n",
    "    col = np.random.choice(df_filtered.columns)\n",
    "    deviation = max_deviation * np.random.random()\n",
    "    df_filtered.loc[index, col] = row[col] + deviation\n",
    "\n",
    "# Create condition indicators based on MCSA and VI analysis techniques\n",
    "df_filtered['BrokenRotorBars'] = abs(df_filtered['current_1']) - abs(df_filtered['current_2'])\n",
    "df_filtered['BearingFault'] = abs(df_filtered['current_2']) - abs(df_filtered['current_3'])\n",
    "df_filtered['Eccentricity'] = abs(df_filtered['current_3']) - abs(df_filtered['current_1'])\n",
    "model = IsolationForest(n_estimators=100, contamination=0.002)\n",
    "model.fit(df_filtered.to_numpy())\n",
    "\n",
    "# Predict the anomalies\n",
    "y_pred = model.predict(df_filtered.to_numpy())\n",
    "indices = np.where(y_pred == -1)[0]\n",
    "indices,anomaly_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5977"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('data/s3Files/data12.txt',header=None,sep=',')\n",
    "df.drop(columns=[0,4],inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.columns = ['current_1', 'current_2', 'current_3']\n",
    "\n",
    "# Create condition indicators based on MCSA and VI analysis techniques\n",
    "df['BrokenRotorBars'] = abs(df['current_1']) - abs(df['current_2'])\n",
    "df['BearingFault'] = abs(df['current_2']) - abs(df['current_3'])\n",
    "df['Eccentricity'] = abs(df['current_3']) - abs(df['current_1'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['current_1', 'current_2', 'current_3', 'BrokenRotorBars',\n",
       "       'BearingFault', 'Eccentricity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply MinMax scaling to normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "df_norm = scaler.fit_transform(df)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(len(df_norm) * 0.8)\n",
    "test_size = len(df_norm) - train_size\n",
    "train, test = df_norm[0:train_size,:], df_norm[train_size:len(df_norm),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(scaler.inverse_transform(test),columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random indices to modify\n",
    "num_anomalies = 10\n",
    "anomaly_indices = np.random.choice(test.index, num_anomalies, replace=False)\n",
    "# Modify the values at the anomaly indices\n",
    "max_deviation = 4\n",
    "for index in anomaly_indices:\n",
    "    row = test.loc[index]\n",
    "    col = np.random.choice(test.columns)\n",
    "    deviation = max_deviation * np.random.random()\n",
    "    test.loc[index, col] = row[col] + deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reshape the data into timesteps for LSTM input\n",
    "def create_dataset(X, look_back=1):\n",
    "    X_data, Y_data = [], []\n",
    "    for i in range(len(X)-look_back-1):\n",
    "        a = X[i:(i+look_back), :]\n",
    "        X_data.append(a)\n",
    "        Y_data.append(X[i + look_back, :])\n",
    "    return np.array(X_data), np.array(Y_data)\n",
    "\n",
    "# Define the number of timesteps and features for LSTM input\n",
    "timesteps = 50\n",
    "features = 6\n",
    "\n",
    "# Reshape the data into timesteps for LSTM input\n",
    "X_train, y_train = create_dataset(train, timesteps)\n",
    "X_test, y_test = create_dataset(test, timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(timesteps, features)))\n",
    "model.add(Dense(features))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 4s 12ms/step - loss: 0.0490\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0073\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0063\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0054\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0047\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0041\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0036\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0032\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0030\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0028\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0026\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0025\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0024\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0024\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0023\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0023\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0022\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0021\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0022\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0021\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0021\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0021\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0020\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0021\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0021\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0020\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.0020\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0020\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0020\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0019\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0019\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0019\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0019\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0019\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0018\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0018\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0019\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0018\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0018\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0018\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 1s 12ms/step - loss: 0.0018\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0018\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2050c7f69d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the trained LSTM model to predict the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  23   62  140  219  257  297  336  414  453  594  673  712  752  790\n",
      "  829  830  890  929 1007 1086 1164 1242 1321 1341 1537 1577 1597 1676\n",
      " 1716 1754 1833 1872]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the reconstruction error for each sample in the test data\n",
    "recon_errors = np.linalg.norm(y_test - y_pred, axis=1)\n",
    "\n",
    "# Define a threshold for anomaly detection\n",
    "threshold = np.mean(recon_errors) + np.std(recon_errors) * 4\n",
    "\n",
    "# Detect the anomalies\n",
    "anomalies = np.where(recon_errors > threshold)[0]\n",
    "\n",
    "# Print the indices of the anomalous samples\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 313,  559,  627,  671,  775,  927, 1150, 1544, 1681, 1811],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(anomaly_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['current_1', 'current_2', 'current_3', 'BrokenRotorBars',\n",
       "       'BearingFault', 'Eccentricity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1438"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\deepdesk\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\deepdesk\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\deepdesk\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2144\\3052777150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Split the data into training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Train a machine learning model to predict future anomalies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\deepdesk\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\deepdesk\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Create features from the data that can be used to train a machine learning model\n",
    "# features = ['current_1', 'current_2', 'current_3', 'BrokenRotorBars', 'BearingFault', 'Eccentricity']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df[features], df['label'], test_size=0.2)\n",
    "\n",
    "# # Train a machine learning model to predict future anomalies\n",
    "# clf = RandomForestClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the testing set and evaluate the model's performance\n",
    "# y_pred = clf.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print('Accuracy:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0398511a9cde84ab83a2fa188ff6508a33d7c0397b3581839dbbf3238a247df4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
